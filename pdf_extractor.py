#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ÁîüÁî£ÁâàPDF‰ªïË®≥ÊäΩÂá∫„Ç∑„Çπ„ÉÜ„É† - Production Ready
PDF„Éï„Ç°„Ç§„É´„Çí5„Éö„Éº„Ç∏„Åö„Å§ÂàÜÂâ≤„Åó„Å¶Claude API„Åß‰ªïË®≥ÊÉÖÂ†±„ÇíÊäΩÂá∫„Åó„ÄÅÁµ±ÂêàCSV„ÇíÂá∫Âäõ„Åô„Çã
Enhanced with production features: retry logic, error handling, logging, configuration management
"""

# Initialize logging first
from logging_config import setup_logging
setup_logging()

import os
import json
import csv
import base64
import logging
import time
import gc
import hashlib
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime
import shutil
import threading
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from dataclasses import dataclass

# Core libraries
import PyPDF2
from PyPDF2 import PdfReader, PdfWriter
import fitz  # PyMuPDF
import pandas as pd

# API clients
try:
    import openai
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

try:
    from anthropic import Anthropic, APIError, RateLimitError
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False

# Local configuration
from config import config, Config

# Set up logger using config
logger = Config.setup_logging()

@dataclass
class ProcessingResult:
    """Result of PDF processing operation"""
    success: bool
    data: List[Dict] = None
    error_message: str = None
    processing_time: float = 0.0
    file_path: str = None
    pages_processed: int = 0
    total_cost_usd: float = 0.0
    total_cost_jpy: float = 0.0

@dataclass 
class APIRetryConfig:
    """Configuration for API retry logic"""
    max_retries: int = 3
    base_delay: float = 1.0
    exponential_backoff: bool = True
    timeout: float = 60.0

def calculate_api_cost(model_name: str, usage) -> float:
    """
    Calculate API cost based on usage and model pricing
    
    Args:
        model_name: Name of the model used
        usage: API usage object with token counts
        
    Returns:
        Cost in USD
    """
    from config import config
    
    model = model_name.lower()
    
    # Determine pricing based on model
    if "gpt-4o" in model:
        if "gpt-4o" in config.API_PRICES:
            price_in = config.API_PRICES["gpt-4o"]["input"]
            price_out = config.API_PRICES["gpt-4o"]["output"]
            return (usage.prompt_tokens / 1000 * price_in + 
                   usage.completion_tokens / 1000 * price_out)
    elif "claude" in model:
        pricing_key = None
        if "claude-sonnet-4" in model or "claude-sonnet-4-0" in model:
            pricing_key = "claude-sonnet-4-0"
        elif "claude-3-5-sonnet" in model:
            pricing_key = "claude-3-5-sonnet"
            
        if pricing_key and pricing_key in config.API_PRICES:
            price_in = config.API_PRICES[pricing_key]["input"]
            price_out = config.API_PRICES[pricing_key]["output"]
            return (usage.input_tokens / 1000 * price_in + 
                   usage.output_tokens / 1000 * price_out)
    
    return 0.0

class ProductionPDFExtractor:
    """Production-ready PDF extractor with enterprise features"""
    
    def __init__(self, api_key: Optional[str] = None, use_mock: Optional[bool] = None):
        """
        Initialize production PDF extractor for Claude Sonnet 4.0
        
        Args:
            api_key: Anthropic API key (overrides config if provided)
            use_mock: Deprecated parameter (ignored, always uses production API)
        """
        # Claude Sonnet 4.0Â∞ÇÁî®
        self.api_provider = 'anthropic'
        self.api_key = api_key or config.ANTHROPIC_API_KEY
        self.model_name = config.ANTHROPIC_MODEL
        self.max_tokens = config.ANTHROPIC_MAX_TOKENS
            
        self.use_mock = False  # „É¢„ÉÉ„ÇØ„É¢„Éº„ÉâÂÆåÂÖ®Êí§ÂªÉ
        
        # Initialize API client (always for production)
        self.client = None
        self._initialize_api_client()
        
        # Processing settings
        self.max_retries = config.MAX_RETRIES
        self.request_timeout = config.REQUEST_TIMEOUT
        self.api_interval = config.API_REQUEST_INTERVAL
        
        # Performance settings
        self.max_concurrent_requests = config.MAX_CONCURRENT_REQUESTS
        self.worker_pool_size = config.WORKER_POOL_SIZE
        
        # File processing settings - 5„Éö„Éº„Ç∏Âçò‰Ωç„ÅßÂÆâÂÆöÂåñ
        self.pages_per_split = 5
        self.max_file_size = config.MAX_FILE_SIZE_MB * 1024 * 1024  # Convert to bytes
        
        # Security settings
        self.mask_api_keys = config.MASK_API_KEYS_IN_LOGS
        
        # Statistics
        self.stats = {
            'files_processed': 0,
            'pages_processed': 0,
            'api_calls_made': 0,
            'errors_encountered': 0,
            'total_processing_time': 0.0,
            'total_cost_usd': 0.0,
            'total_cost_jpy': 0.0
        }
        
        logger.info(f"ProductionPDFExtractor initialized - Provider: {self.api_provider}, Mock: {self.use_mock}")
    
    def _initialize_api_client(self) -> None:
        """Initialize Anthropic API client with validation"""
        if not self.api_key or self.api_key == 'DUMMY_API_KEY':
            raise ValueError("Valid Anthropic API key required for production use")
        
        try:
            if not HAS_ANTHROPIC:
                raise ImportError("Anthropic library not installed. Install with: pip install anthropic")
            
            # Claude Sonnet 4.0Áî®„ÅÆ„ÇØ„É©„Ç§„Ç¢„É≥„ÉàÂàùÊúüÂåñ
            client_kwargs = {"api_key": self.api_key}
            
            # „Éô„Éº„Çø„Éò„ÉÉ„ÉÄ„Éº„ÅÆË®≠ÂÆö
            if hasattr(config, 'ANTHROPIC_BETA_HEADERS') and config.ANTHROPIC_BETA_HEADERS:
                client_kwargs["default_headers"] = {"anthropic-beta": config.ANTHROPIC_BETA_HEADERS}
            
            self.client = Anthropic(**client_kwargs)
            logger.info(f"Claude Sonnet 4.0 API client initialized successfully - Model: {self.model_name}")
            
            # Test API connectivity
            if config.DEBUG_MODE:
                self._test_api_connectivity()
                
        except Exception as e:
            logger.error(f"Failed to initialize Claude API client: {e}")
            raise
    
    def _test_api_connectivity(self) -> bool:
        """Test Claude API connectivity with a simple request"""
        try:
            response = self.client.messages.create(
                model=self.model_name,
                max_tokens=10,
                messages=[{"role": "user", "content": "Hello"}]
            )
            logger.info(f"Claude Sonnet 4.0 API connectivity test successful - Model: {self.model_name}")
            return True
        except Exception as e:
            logger.warning(f"Claude API connectivity test failed: {e}")
            return False
    
    def validate_file(self, pdf_path: Path) -> Tuple[bool, str]:
        """
        Validate PDF file before processing
        
        Args:
            pdf_path: Path to PDF file
            
        Returns:
            Tuple of (is_valid, error_message)
        """
        try:
            if not pdf_path.exists():
                return False, f"File does not exist: {pdf_path}"
            
            if not pdf_path.is_file():
                return False, f"Path is not a file: {pdf_path}"
            
            # Check file extension
            if pdf_path.suffix.lower() != '.pdf':
                return False, f"File is not a PDF: {pdf_path}"
            
            # Check file size
            file_size = pdf_path.stat().st_size
            if file_size > self.max_file_size:
                return False, f"File too large: {file_size} bytes (max: {self.max_file_size})"
            
            # Try to open and read PDF
            with open(pdf_path, 'rb') as file:
                reader = PdfReader(file)
                page_count = len(reader.pages)
                
                if page_count == 0:
                    return False, "PDF contains no pages"
                
                # Check if PDF is encrypted
                if reader.is_encrypted:
                    return False, "PDF is password protected"
            
            logger.info(f"File validation passed: {pdf_path.name} ({page_count} pages, {file_size} bytes)")
            return True, ""
            
        except Exception as e:
            return False, f"File validation error: {str(e)}"
    
    def backup_file(self, pdf_path: Path) -> Optional[Path]:
        """Create backup of original file if enabled"""
        if not config.BACKUP_ORIGINAL_FILES:
            return None
        
        try:
            backup_dir = config.OUTPUT_DIR / "backups"
            backup_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{pdf_path.stem}_{timestamp}{pdf_path.suffix}"
            backup_path = backup_dir / backup_name
            
            shutil.copy2(pdf_path, backup_path)
            logger.info(f"File backed up: {backup_path}")
            
            return backup_path
        except Exception as e:
            logger.warning(f"Failed to backup file: {e}")
            return None
    
    def split_pdf(self, pdf_path: Path, output_dir: Path, pages_per_split: int = 5) -> List[Path]:
        """
        Split PDF with enhanced error handling and validation
        
        Args:
            pdf_path: Source PDF file path
            output_dir: Output directory for split files
            pages_per_split: Number of pages per split
            
        Returns:
            List of split file paths
        """
        logger.info(f"Starting PDF split: {pdf_path.name}")
        
        output_dir.mkdir(parents=True, exist_ok=True)
        split_files = []
        
        try:
            with open(pdf_path, 'rb') as file:
                reader = PdfReader(file)
                total_pages = len(reader.pages)
                
                logger.info(f"Total pages: {total_pages}")
                
                if total_pages == 0:
                    raise ValueError("PDF contains no pages")
                
                for start_page in range(0, total_pages, pages_per_split):
                    end_page = min(start_page + pages_per_split, total_pages)
                    
                    try:
                        writer = PdfWriter()
                        
                        # Add pages to writer
                        for page_num in range(start_page, end_page):
                            writer.add_page(reader.pages[page_num])
                        
                        # Generate output filename
                        output_filename = f"{pdf_path.stem}_pages_{start_page+1}-{end_page}.pdf"
                        output_path = output_dir / output_filename
                        
                        # Write split file
                        with open(output_path, 'wb') as output_file:
                            writer.write(output_file)
                        
                        split_files.append(output_path)
                        logger.info(f"Split created: {output_filename} (pages {start_page+1}-{end_page})")
                        
                    except Exception as e:
                        logger.error(f"Failed to create split {start_page+1}-{end_page}: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"PDF split failed: {pdf_path} - {e}")
            # Clean up any partial split files
            for split_file in split_files:
                try:
                    if split_file.exists():
                        split_file.unlink()
                except:
                    pass
            raise
            
        return split_files
    
    def pdf_to_images_base64(self, pdf_path: Path) -> List[str]:
        """
        Convert PDF to base64 images with enhanced error handling
        
        Args:
            pdf_path: PDF file path
            
        Returns:
            List of base64 encoded images
        """
        images = []
        doc = None
        
        try:
            doc = fitz.open(pdf_path)
            
            for page_num in range(len(doc)):
                try:
                    page = doc.load_page(page_num)
                    
                    # High resolution conversion
                    mat = fitz.Matrix(config.PDF_DPI / 72.0, config.PDF_DPI / 72.0)
                    pix = page.get_pixmap(matrix=mat)
                    
                    # Convert to PNG bytes
                    img_data = pix.tobytes("png")
                    
                    # Base64 encode
                    img_b64 = base64.b64encode(img_data).decode('utf-8')
                    images.append(img_b64)
                    
                    logger.debug(f"Converted page {page_num + 1} to image ({len(img_data)} bytes)")
                    
                    # Memory cleanup
                    pix = None
                    
                except Exception as e:
                    logger.error(f"Failed to convert page {page_num + 1}: {e}")
                    continue
                    
            return images
            
        except Exception as e:
            logger.error(f"PDF to images conversion failed: {pdf_path} - {e}")
            raise
        finally:
            if doc:
                doc.close()
    
    def extract_with_retry(self, pdf_path: Path, page_start: int, page_end: int, 
                          retry_config: Optional[APIRetryConfig] = None) -> List[Dict]:
        """
        Extract data with single attempt (no retry to save API costs)
        
        Args:
            pdf_path: PDF file path
            page_start: Start page number
            page_end: End page number  
            retry_config: Retry configuration (ignored - kept for compatibility)
            
        Returns:
            Extracted data list
        """
        try:
            # Perform single extraction attempt
            result = self._extract_from_pdf_chunk_internal(pdf_path, page_start, page_end)
            
            if result is not None:
                self.stats['api_calls_made'] += 1
                return result
            else:
                logger.error(f"Extraction returned None for {pdf_path.name} pages {page_start}-{page_end}")
                self.stats['errors_encountered'] += 1
                return []
                    
        except Exception as e:
            # Single attempt failed - return empty result immediately
            logger.error(f"API extraction failed for {pdf_path.name} pages {page_start}-{page_end}: {e}")
            self.stats['errors_encountered'] += 1
            return []
    
    def _extract_from_pdf_chunk_internal(self, pdf_path: Path, page_start: int, page_end: int) -> List[Dict]:
        """Internal extraction method with timeout handling"""
        filename = pdf_path.name
        page_range = f"{page_start}-{page_end}"
        
        logger.info(f"Extracting: {filename} (pages {page_range}) [Êú¨Áï™API‰ΩøÁî®]")
        
        # Create extraction task with timeout
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(self._perform_api_extraction, pdf_path, filename, page_range)
            
            try:
                result = future.result(timeout=self.request_timeout)
                return result
            except TimeoutError:
                logger.error(f"API request timed out after {self.request_timeout}s")
                raise
    
    def _perform_api_extraction(self, pdf_path: Path, filename: str, page_range: str) -> List[Dict]:
        """Perform the actual API extraction"""
        try:
            # Convert PDF to images
            images_b64 = self.pdf_to_images_base64(pdf_path)
            if not images_b64:
                logger.warning(f"No images extracted from {filename}")
                return []
            
            # System„Éó„É≠„É≥„Éó„Éà‰ΩøÁî®ÔºàÂõ∫ÂÆöÂåñÔºâ
            system_prompt = self._get_system_prompt()
            
            # „É¶„Éº„Ç∂„Éº„Éó„É≠„É≥„Éó„Éà„Å´Âàó„Éí„É≥„ÉàÊñá„ÇíËøΩÂä†
            user_prompt = f"„Éï„Ç°„Ç§„É´Âêç: {filename}, „Éö„Éº„Ç∏: {page_range}\n"
            for page_idx in range(1, len(images_b64) + 1):
                user_prompt += f"„Éö„Éº„Ç∏{page_idx}ÔºöÂ∑¶=ÂÄüÊñπÔΩú‰∏≠Â§Æ=ÊëòË¶ÅÔΩúÂè≥=Ë≤∏ÊñπÔºàÂàó„ÇíÂèñ„ÇäÈÅï„Åà„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑÔºâ\n"
            
            # Claude Sonnet 4.0 APIÂëº„Å≥Âá∫„ÅóÔºàJSONÊäΩÂá∫„Ç¨„Éº„Éâ‰ªò„ÅçÔºâ
            response_text, cost_usd = self._call_claude_api(system_prompt, user_prompt, images_b64)
            
            # JSONÊäΩÂá∫„Ç¨„Éº„Éâ„Çí‰ΩøÁî®„Åó„Å¶„Éë„Éº„Çπ
            from utils.json_guard import parse_5cols_json, get_fallback_entry
            try:
                extracted_data = parse_5cols_json(response_text)
            except Exception as e:
                logger.error(f"JSONÊäΩÂá∫„Ç¨„Éº„Éâ„Åß„Ç®„É©„Éº: {e}")
                logger.info(f"JSONÊäΩÂá∫Â§±Êïó - ÂéüÂõ†: {type(e).__name__}: {str(e)[:100]}")
                # 1Âõû„ÅÆ„Åø„É™„Éà„É©„Ç§ÔºàÁü≠Á∏Æ„Éó„É≠„É≥„Éó„ÉàÔºâ
                logger.info("Áü≠Á∏Æ„Éó„É≠„É≥„Éó„Éà„Åß1Âõû„É™„Éà„É©„Ç§")
                try:
                    retry_system = "Âá∫Âäõ„ÅØJSONÈÖçÂàó„ÅÆ„Åø„ÄÇÂêÑË¶ÅÁ¥†„ÅØ5„Ç´„É©„É†Ôºà‰ºùÁ•®Êó•‰ªò/ÂÄüË≤∏Âå∫ÂàÜ/ÁßëÁõÆÂêç/ÈáëÈ°ç/ÊëòË¶ÅÔºâ„ÄÇ‰ΩôË®à„Å™ÊñáÂ≠ó„Éª„Ç≥„Éº„Éâ„Éï„Çß„É≥„ÇπÁ¶ÅÊ≠¢„ÄÇ"
                    retry_response, retry_cost = self._call_claude_api(retry_system, user_prompt, images_b64)
                    cost_usd += retry_cost
                    extracted_data = parse_5cols_json(retry_response)
                    logger.info(f"„É™„Éà„É©„Ç§ÊàêÂäü: {len(extracted_data)}„Ç®„É≥„Éà„É™")
                except Exception as retry_e:
                    logger.error(f"„É™„Éà„É©„Ç§„ÇÇÂ§±Êïó: {retry_e}")
                    logger.info(f"JSONÊäΩÂá∫Â§±Êïó - ÂéüÂõ†: „É™„Éà„É©„Ç§Âæå„ÇÇ„Éë„Éº„ÇπÂ§±Êïó ({type(retry_e).__name__})")
                    extracted_data = get_fallback_entry("ÊäΩÂá∫‰∏çËÉΩ")
            
            # 5„Ç´„É©„É†„ÅÆÂâçÊÆµÊï¥ÂΩ¢Ôºà„Ç≥„Éº„ÉâÂàó„ÇíË¶ã„Å™„ÅÑÊÆµÈöé„Åß„ÅÆÈáçË§á„ÉªÂΩ¢Áä∂Êï¥ÁêÜÔºâ
            from utils.reconcile_entries import reconcile_entries
            # ========== DIAG: stage2_reconcile ==========
            stage2_before = len(extracted_data)
            extracted_data, reconcile_metrics = reconcile_entries(extracted_data, sum_tolerance=0, return_metrics=True)
            stage2_count = len(extracted_data)
            logger.info(f"DIAG stage2_reconcile: count={stage2_count} (before={stage2_before}, diff={stage2_count-stage2_before})")
            logger.info(f"DIAG stage2_reconcile metrics: splits={reconcile_metrics['split_count']}, swaps={reconcile_metrics['swap_count']}, drops={reconcile_metrics['drop_count']}")
            if stage2_count == 0:
                logger.error("DIAG stage2_reconcile=0: ÂâçÊÆµÊï¥ÂΩ¢„ÅßÂÖ®ÂâäÈô§")
                raise RuntimeError("DIAG stage2_reconcile=0: ÂâçÊÆµÊï¥ÂΩ¢Âá¶ÁêÜ„Åß„Ç®„É≥„Éà„É™„ÅåÂÖ®„Å¶ÂâäÈô§„Åï„Çå„Åæ„Åó„Åü„ÄÇ")
            
            # ÂæåÂá¶ÁêÜ: Ë≤∏ÂÄü„Éö„Ç¢‰øùË®º„Å®ÈáëÈ°ç„Éê„É™„Éá„Éº„Ç∑„Éß„É≥
            from utils.postprocess import enforce_debit_credit_pairs, validate_amounts
            from config import config
            paired_entries = enforce_debit_credit_pairs(extracted_data)
            # ========== DIAG: stage3_pair_validate ==========
            stage3_before = len(paired_entries)
            extracted_data, error_entries = validate_amounts(paired_entries)
            stage3_count = len(extracted_data)
            logger.info(f"DIAG stage3_pair_validate: count={stage3_count} (before={stage3_before}, errors={len(error_entries)})")
            if stage3_count == 0:
                logger.error("DIAG stage3_pair_validate=0: ÈáëÈ°ç„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„ÅßÂÖ®ÂâäÈô§")
                raise RuntimeError("DIAG stage3_pair_validate=0: ÈáëÈ°ç„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„Åß„Ç®„É≥„Éà„É™„ÅåÂÖ®„Å¶ÂâäÈô§„Åï„Çå„Åæ„Åó„Åü„ÄÇ")
            
            if error_entries:
                logger.warning(f"Post-processing found {len(error_entries)} error entries (zero amounts, etc.)")
                # „Ç®„É©„Éº„Ç®„É≥„Éà„É™„ÅÆÊÉÖÂ†±„Çí„É≠„Ç∞„Å´Ë®òÈå≤ÔºàUIË°®Á§∫Áî®Ôºâ
                for err in error_entries[:3]:  # ÊúÄÂàù„ÅÆ3‰ª∂„ÅÆ„Åø„É≠„Ç∞Âá∫Âäõ
                    logger.warning(f"Error entry: Êó•‰ªò={err.get('‰ºùÁ•®Êó•‰ªò', '')}, ÈáëÈ°ç={err.get('ÈáëÈ°ç', 0)}, ÊëòË¶Å={err.get('ÊëòË¶Å', '')[:50]}...")
                if len(error_entries) > 3:
                    logger.warning(f"... and {len(error_entries) - 3} more error entries")
                
                # „Ç®„É©„ÉºÊÉÖÂ†±„Çístats„Å´‰øùÂ≠òÔºàUIË°®Á§∫Áî®Ôºâ
                if not hasattr(self, 'error_entries'):
                    self.error_entries = []
                self.error_entries.extend(error_entries)
            
            # Áµ±Ë®àÊõ¥Êñ∞
            self.stats['total_cost_usd'] += cost_usd
            current_usd_rate = config.get_current_usd_to_jpy_rate()
            cost_jpy = cost_usd * current_usd_rate
            self.stats['total_cost_jpy'] += cost_jpy
            
            # ÊàêÂäü„É≠„Ç∞ÔºàË©≥Á¥∞ÊÉÖÂ†±‰ªò„ÅçÔºâ
            logger.info(f"ÊäΩÂá∫ÊàêÂäü: {len(extracted_data)}„Ç®„É≥„Éà„É™, Ë≤ªÁî®: ${cost_usd:.4f} USD (¬•{cost_jpy:.2f} JPY)")
            logger.debug(f"ÊäΩÂá∫„Éá„Éº„Çø„Çµ„É≥„Éó„É´: {self._sanitize_extracted_data_for_logging(extracted_data[:2])}")
            return extracted_data
            
        except Exception as e:
            logger.error(f"API extraction failed: {filename} - {e}")
            raise
    
    def _sanitize_for_logging(self, text: str) -> str:
        """
        „É≠„Ç∞Âá∫ÂäõÁî®„Å´PIIÔºàÂÄã‰∫∫Ë≠òÂà•ÊÉÖÂ†±Ôºâ„Çí„Éû„Çπ„Ç≠„É≥„Ç∞
        
        Args:
            text: ÂÖÉ„ÅÆ„ÉÜ„Ç≠„Çπ„Éà
            
        Returns:
            str: „Éû„Çπ„Ç≠„É≥„Ç∞Âæå„ÅÆ„ÉÜ„Ç≠„Çπ„Éà
        """
        from utils.masking import mask_personal_info
        return mask_personal_info(text)
        
    def _sanitize_extracted_data_for_logging(self, data: List[Dict]) -> List[Dict]:
        """
        ÊäΩÂá∫„Éá„Éº„Çø„ÅÆ„É≠„Ç∞Âá∫ÂäõÁî®„Çµ„Éã„Çø„Ç§„Ç∫
        
        Args:
            data: ÊäΩÂá∫„Éá„Éº„Çø
            
        Returns:
            List[Dict]: „Éû„Çπ„Ç≠„É≥„Ç∞Âæå„ÅÆ„Éá„Éº„Çø
        """
        from utils.masking import mask_list_for_logging
        return mask_list_for_logging(data, sample_size=2)
    
    def _call_claude_api(self, system_prompt: str, user_prompt: str, images_b64: List[str]) -> Tuple[str, float]:
        """Call Claude Sonnet 4.0 API with system prompt and images"""
        try:
            # Build user message
            message = {
                "role": "user",
                "content": [{"type": "text", "text": user_prompt}]
            }
            
            # Add images
            for img_b64 in images_b64:
                message["content"].append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/png", 
                        "data": img_b64
                    }
                })
            
            logger.info(f"Making Claude API call with model: {self.model_name}")
            
            # JSON Schema for forced object array output
            response_format = {
                "type": "json_schema",
                "json_schema": {
                    "name": "five_columns",
                    "schema": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "required": ["‰ºùÁ•®Êó•‰ªò", "ÂÄüË≤∏Âå∫ÂàÜ", "ÁßëÁõÆÂêç", "ÈáëÈ°ç", "ÊëòË¶Å"],
                            "properties": {
                                "‰ºùÁ•®Êó•‰ªò": {"type": "string"},
                                "ÂÄüË≤∏Âå∫ÂàÜ": {"type": "string", "enum": ["ÂÄüÊñπ", "Ë≤∏Êñπ"]},
                                "ÁßëÁõÆÂêç": {"type": "string"},
                                "ÈáëÈ°ç": {"type": ["string", "number"]},
                                "ÊëòË¶Å": {"type": "string"}
                            },
                            "additionalProperties": False
                        }
                    }
                }
            }
            
            # Make API call with stabilized settings (response_format removed for compatibility)
            try:
                # Try with response_format first (for future compatibility)
                try:
                    response = self.client.messages.create(
                        model=self.model_name,
                        max_tokens=4096,  # ÂÆâÂÆöÂåñ: ÂàáÊñ≠„ÇíÈÅø„Åë„Çã
                        temperature=0,    # ÂÆâÂÆöÂåñ: Á≤æÂ∫¶ÈáçË¶ñ„ÄÅÂÜçÁèæÊÄßÁ¢∫‰øù
                        system=system_prompt,  # System„Éó„É≠„É≥„Éó„Éà„Çí„Ç∑„Çπ„ÉÜ„É†„Å´Ë®≠ÂÆö
                        messages=[message],
                        timeout=self.request_timeout,
                        response_format=response_format  # JSON SchemaÂº∑Âà∂ÔºàÂ∞ÜÊù•ÂØæÂøúÊôÇÔºâ
                    )
                    logger.info("JSON SchemaÂº∑Âà∂„É¢„Éº„Éâ: ÊàêÂäü")
                except (TypeError, AttributeError) as schema_error:
                    logger.info(f"JSON SchemaÈùûÂØæÂøú - ÂæìÊù•„É¢„Éº„Éâ„ÅßÁ∂öË°å: {type(schema_error).__name__}")
                    # Fallback to normal API call without response_format
                    response = self.client.messages.create(
                        model=self.model_name,
                        max_tokens=4096,  # ÂÆâÂÆöÂåñ: ÂàáÊñ≠„ÇíÈÅø„Åë„Çã
                        temperature=0,    # ÂÆâÂÆöÂåñ: Á≤æÂ∫¶ÈáçË¶ñ„ÄÅÂÜçÁèæÊÄßÁ¢∫‰øù
                        system=system_prompt,  # System„Éó„É≠„É≥„Éó„Éà„Çí„Ç∑„Çπ„ÉÜ„É†„Å´Ë®≠ÂÆö
                        messages=[message],
                        timeout=self.request_timeout
                    )
            except Exception as e:
                logger.error(f"Claude API call failed: {e}")
                raise
            
            # usage ÊÉÖÂ†±„Çí„É≠„Ç∞„Å´ËøΩÂä†„Å®Ë≤ªÁî®Ë®àÁÆó
            cost_usd = 0.0
            try:
                if hasattr(response, "usage") and response.usage:
                    logger.info(f"‰ΩøÁî®„Éà„Éº„ÇØ„É≥Êï∞: input={response.usage.input_tokens}, "
                              f"output={response.usage.output_tokens}, "
                              f"total={response.usage.input_tokens + response.usage.output_tokens}")
                    
                    # Ë≤ªÁî®Ë®àÁÆó
                    cost_usd = calculate_api_cost(self.model_name, response.usage)
                    current_usd_rate = config.get_current_usd_to_jpy_rate()
                    cost_jpy = cost_usd * current_usd_rate
                    logger.info(f"Êé®ÂÆöË≤ªÁî®: ${cost_usd:.4f} USD (¬•{cost_jpy:.2f} JPY) [„É¨„Éº„Éà: {current_usd_rate:.2f}]")
                    
                    # Áµ±Ë®à„Å´ËøΩÂä†
                    self.stats['total_cost_usd'] += cost_usd
                    self.stats['total_cost_jpy'] += cost_jpy
                else:
                    logger.warning("API response does not contain usage information")
            except Exception as e:
                logger.error(f"Error processing usage information: {e}")
                cost_usd = 0.0
            
            response_text = response.content[0].text.strip()
            
            # „É¨„Çπ„Éù„É≥„ÇπÂÜÖÂÆπ„Çí„É≠„Ç∞„Å´Ë®òÈå≤Ôºà„Éá„Éê„ÉÉ„Ç∞Áî®„ÉªPIIÈÖçÊÖÆÔºâ
            sanitized_preview = self._sanitize_for_logging(response_text[:8000])  # ÊúÄÂ§ß8KB
            logger.debug(f"Claude API Response length: {len(response_text)} characters")
            logger.debug(f"Claude API Response preview: {sanitized_preview[:500]}...")
            
            # Êé®ÂÆöÁµêÊûú‰ª∂Êï∞„Çí„É≠„Ç∞„Å´Ë®òÈå≤
            estimated_entries = response_text.count('"{') if response_text else 0
            logger.info(f"Claude APIÊé®ÂÆöÁµêÊûú: {estimated_entries}‰ª∂„ÅÆ„Ç®„É≥„Éà„É™ÂÄôË£ú")
            
            # „Éá„Éê„ÉÉ„Ç∞Áî®: „É¨„Çπ„Éù„É≥„Çπ„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò
            if config.DEBUG_MODE:
                debug_dir = config.LOG_DIR / "debug_responses"
                debug_dir.mkdir(parents=True, exist_ok=True)
                debug_file = debug_dir / f"claude_response_{int(time.time())}.txt"
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(f"Model: {self.model_name}\n")
                    f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                    f.write(f"Cost: ${cost_usd:.4f} USD\n")
                    f.write("="*50 + "\n")
                    f.write(response_text)
                logger.info(f"Claude API response saved to: {debug_file}")
            
            return response_text, cost_usd
            
        except Exception as e:
            logger.error(f"Claude API call failed: {e}")
            raise
    
    def _clean_json_response(self, response_text: str) -> str:
        """Clean API response to extract valid JSON"""
        # Remove code block markers if present
        if "```json" in response_text:
            start = response_text.find("```json") + 7
            end = response_text.find("```", start)
            if end != -1:
                response_text = response_text[start:end].strip()
        elif "```" in response_text:
            start = response_text.find("```") + 3
            end = response_text.rfind("```")
            if end != -1 and end > start:
                response_text = response_text[start:end].strip()
        
        # Remove any leading/trailing text that's not JSON
        response_text = response_text.strip()
        start_bracket = response_text.find('[')
        end_bracket = response_text.rfind(']')
        
        if start_bracket != -1 and end_bracket != -1 and end_bracket > start_bracket:
            response_text = response_text[start_bracket:end_bracket + 1]
        
        return response_text
    
    def _attempt_json_recovery(self, response_text: str, filename: str, page_range: str) -> List[Dict]:
        """Attempt to recover data from malformed JSON response"""
        logger.info("Attempting JSON recovery...")
        
        try:
            # Try to find individual JSON objects
            import re
            
            # Look for objects enclosed in curly braces
            object_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
            matches = re.findall(object_pattern, response_text, re.DOTALL)
            
            recovered_objects = []
            for match in matches:
                try:
                    obj = json.loads(match)
                    if isinstance(obj, dict) and any(key in obj for key in config.REQUIRED_FIELDS):
                        recovered_objects.append(obj)
                except:
                    continue
            
            if recovered_objects:
                logger.info(f"Recovered {len(recovered_objects)} objects from malformed JSON")
                return self._validate_extracted_data(recovered_objects, filename, page_range)
            
        except Exception as e:
            logger.error(f"JSON recovery failed: {e}")
        
        # Return empty list if recovery fails
        return []
    
    def _validate_extracted_data(self, data: List[Dict], filename: str, page_range: str) -> List[Dict]:
        """Validate and clean extracted data"""
        if not data:
            return []
        
        validated_data = []
        
        for i, entry in enumerate(data):
            try:
                # Ensure entry is a dictionary
                if not isinstance(entry, dict):
                    logger.warning(f"Entry {i} is not a dictionary, skipping")
                    continue
                
                # Validate required fields exist (5„Ç´„É©„É†JSON)
                missing_fields = [field for field in config.REQUIRED_JSON_FIELDS if field not in entry]
                for field in missing_fields:
                    entry[field] = "" if field != "ÈáëÈ°ç" else 0
                
                # Validate amount field
                if config.VALIDATE_AMOUNTS and entry.get("ÈáëÈ°ç"):
                    try:
                        amount = float(str(entry["ÈáëÈ°ç"]).replace(",", ""))
                        if not (config.MIN_AMOUNT <= amount <= config.MAX_AMOUNT):
                            logger.warning(f"Amount {amount} out of valid range, entry {i}")
                    except (ValueError, TypeError):
                        logger.warning(f"Invalid amount format in entry {i}: {entry.get('ÈáëÈ°ç')}")
                
                # 5„Ç´„É©„É†JSON„Å´„ÅØÂèÇÁÖßÂÖÉ„Éï„Ç°„Ç§„É´/„Éö„Éº„Ç∏„ÅØÂê´„Åæ„Å™„ÅÑÔºàMJSÂ§âÊèõ„Åß‰ªò‰∏éÔºâ
                
                validated_data.append(entry)
                
            except Exception as e:
                logger.error(f"Error validating entry {i}: {e}")
                continue
        
        logger.info(f"Validated {len(validated_data)} entries out of {len(data)}")
        return validated_data
    
    def get_enhanced_mock_data(self, filename: str, page_range: str) -> List[Dict]:
        """Enhanced mock data for testing"""
        mock_entries = [
            {
                "Â•ëÁ¥ÑÊó•": "R07/02/04",
                "ÂÄüÊñπÁßëÁõÆ": "Ê∞¥ÈÅìÂÖâÁÜ±Ë≤ª",
                "Ë≤∏ÊñπÁßëÁõÆ": "‰∏âËè±UFJÊôÆÈÄöÈ†êÈáë",
                "ÊëòË¶Å": "„ÇØ„É™„Çπ„ÇØ„É¨„É°„É≥„ÉàÂÖ±Áî® Ê∞¥ÈÅìÊñô",
                "ÈáëÈ°ç": 3014,
                "ÂÇôËÄÉ": "Â•ëÁ¥ÑËÄÖÂêç: ; Âè∑ÂÆ§: ; Áâ©‰ª∂Âêç: ; „Ç™„Éº„Éä„Éº: ",
                "ÂèÇÁÖßÂÖÉ„Éï„Ç°„Ç§„É´": filename,
                "„Éö„Éº„Ç∏": page_range
            },
            {
                "Â•ëÁ¥ÑÊó•": "R07/02/05",
                "ÂÄüÊñπÁßëÁõÆ": "È†ê„ÇäÈáë",
                "Ë≤∏ÊñπÁßëÁõÆ": "‰∏âËè±ÊôÆÈÄö",
                "ÊëòË¶Å": "„Ç∏„Çß„Ç§„É™„Éº„Çπ (Ê†™) ‰øùË®ºÊñô",
                "ÈáëÈ°ç": 12000,
                "ÂÇôËÄÉ": "Â•ëÁ¥ÑËÄÖÂêç: Â±±‰∏ãËâØ‰∏â; Âè∑ÂÆ§: No.13; Áâ©‰ª∂Âêç: „Éï„Ç£„Éº„É´„Éâ„Éë„Éº„Ç≠„É≥„Ç∞; „Ç™„Éº„Éä„Éº: ÊûóÁØ§Âè≤",
                "ÂèÇÁÖßÂÖÉ„Éï„Ç°„Ç§„É´": filename,
                "„Éö„Éº„Ç∏": page_range
            }
        ]
        
        # Add randomization for testing
        if config.DEBUG_MODE:
            import random
            base_amount = random.randint(1000, 50000)
            mock_entries.append({
                "Â•ëÁ¥ÑÊó•": f"R07/02/{random.randint(1, 28):02d}",
                "ÂÄüÊñπÁßëÁõÆ": random.choice(["ÁèæÈáë", "ÊôÆÈÄöÈ†êÈáë", "Â£≤ÊéõÈáë"]),
                "Ë≤∏ÊñπÁßëÁõÆ": random.choice(["Â£≤‰∏äÈ´ò", "ÂèóÂèñÊâãÊï∞Êñô", "È†ê„ÇäÈáë"]),
                "ÊëòË¶Å": f"„ÉÜ„Çπ„ÉàÂèñÂºï #{random.randint(1000, 9999)}",
                "ÈáëÈ°ç": base_amount,
                "ÂÇôËÄÉ": f"„ÉÜ„Çπ„Éà„Éá„Éº„Çø - {filename}",
                "ÂèÇÁÖßÂÖÉ„Éï„Ç°„Ç§„É´": filename,
                "„Éö„Éº„Ç∏": page_range
            })
        
        return mock_entries
    
    def _get_system_prompt(self) -> str:
        """Âº∑Âåñ„Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„ÉàÔºà„Ç™„Éñ„Ç∏„Çß„ÇØ„ÉàÈÖçÂàóÂº∑Âà∂„ÉªË≤∏ÂÄü„Éö„Ç¢‰øùË®º„ÉªÊëòË¶ÅÁµ±‰∏ÄÔºâ"""
        return """„ÅÇ„Å™„Åü„ÅØ„ÄåÂ∏≥Á•®OCRÂ§âÊèõÂô®ÔºàÁõ£Êüª‰∫∫Ë¶ñÁÇπÔºâ„Äç„Åß„Åô„ÄÇA4„ÅÆ‰∏çÂãïÁî£‰ºùÁ•®PDFÔºàÈÄÄÂéª„ÉªÊåØÊõø„ÉªÊõ¥Êñ∞„ÉªÊñ∞Ë¶è„Å™„Å©Ôºâ„Åã„Çâ„Äê„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅÆÈÖçÂàó„Äë„ÇíJSONÂΩ¢Âºè„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêÂá∫ÂäõÂΩ¢ÂºèÔºàÂé≥ÂÆàÔºâ„Äë
- Âá∫Âäõ„ÅØ JSON „Åß„ÄÅ**„Ç™„Éñ„Ç∏„Çß„ÇØ„ÉàÔºàËæûÊõ∏Ôºâ„ÅÆÈÖçÂàó**„ÅÆ„Åø„ÄÇ
- ÂêÑ„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅØ **ÂøÖÈ†à5„Ç≠„Éº**Ôºö["‰ºùÁ•®Êó•‰ªò","ÂÄüË≤∏Âå∫ÂàÜ","ÁßëÁõÆÂêç","ÈáëÈ°ç","ÊëòË¶Å"]
- ‚òÖÈáçË¶Å‚òÖ 5Ë¶ÅÁ¥†„ÅÆÈÖçÂàó ["2024/1/1", "ÂÄüÊñπ", "ÁèæÈáë", 1000, "ÊëòË¶Å"] „ÇÑ„Çø„Éó„É´„ÅßËøî„Åó„Å¶„ÅØ„Å™„Çâ„Å™„ÅÑ„ÄÇÂøÖ„Åö { "‰ºùÁ•®Êó•‰ªò": "...", "ÂÄüË≤∏Âå∫ÂàÜ": "...", "ÁßëÁõÆÂêç": "...", "ÈáëÈ°ç": ..., "ÊëòË¶Å": "..." } „ÅÆËæûÊõ∏„Ç™„Éñ„Ç∏„Çß„ÇØ„ÉàÂΩ¢Âºè„ÄÇ
- ‰ΩôË®à„Å™Ë™¨ÊòéÊñá„Éª„Ç≥„Éº„Éâ„Éï„Çß„É≥„Çπ„ÉªmarkdownË®òÊ≥ï„ÅØ‰∏ÄÂàáÁ¶ÅÊ≠¢„ÄÇ

„ÄêÂá∫Âäõ„Ç´„É©„É†Ôºà5„Ç≠„ÉºÂøÖÈ†àÔºâ„Äë
1) "‰ºùÁ•®Êó•‰ªò"ÔºàYYYY/M/D„ÄÅË•øÊö¶ÊñáÂ≠óÂàóÔºâ
2) "ÂÄüË≤∏Âå∫ÂàÜ"Ôºà"ÂÄüÊñπ" „Åæ„Åü„ÅØ "Ë≤∏Êñπ"Ôºâ
3) "ÁßëÁõÆÂêç"Ôºà‰∏çÊòé„Å™„ÇâÁ©∫ÊñáÂ≠ó„Åß„Çà„ÅÑÔºâ
4) "ÈáëÈ°ç"ÔºàÂçäËßíÊï∞Â≠ó„ÄÅÊ≠£Êï¥Êï∞Ôºâ
5) "ÊëòË¶Å"ÔºàÂÖ±ÈÄöÊëòË¶ÅÔºãË°åÂõ∫ÊúâÔºã„Äå; ÂÄüÊñπ:XXX / Ë≤∏Êñπ:YYY„ÄçÔºâ

„Äê„É¨„Ç§„Ç¢„Ç¶„Éà„ÅÆÁµ∂ÂØæÂâá„Äë
- ‰ºùÁ•®„ÅØ„ÄåÂ∑¶=ÂÄüÊñπÔºèÂè≥=Ë≤∏Êñπ„Äç„ÄÇ‰∏≠Â§Æ„Å´ÂÖ±ÈÄöÊëòË¶Å„ÄÇ
- ÊØé„ÄåÊû†„ÄçÔºàË°åÂ∏ØÔºâ„Å´„Å§„ÅÑ„Å¶„ÄÅÂøÖ„ÅöÂÄüÊñπ„É¨„ÉÉ„Ç∞„Å®Ë≤∏Êñπ„É¨„ÉÉ„Ç∞„ÅÆ„Éö„Ç¢„ÇíÂá∫Âäõ„Åô„Çã„ÄÇ
  - ÁâáÂÅ¥„ÅåË™≠„ÇÅ„Å™„ÅÑÂ†¥ÂêàÔºöÂÄüË≤∏Âå∫ÂàÜ„ÅØÊ≠£„Åó„Åè„ÄÅÈáëÈ°ç„ÅØÊû†„ÅÆÈáëÈ°ç„ÄÅ‰∏çÊòéÂÅ¥„ÅÆÁßëÁõÆÂêç„ÅØÁ©∫ÊñáÂ≠ó„Åß„Çà„ÅÑ„ÄÇ
  - „ÅÑ„Åö„Çå„ÅÆÂ†¥Âêà„ÇÇÊëòË¶ÅÊú´Â∞æ„Å´„Äå; ÂÄüÊñπ:XXX / Ë≤∏Êñπ:YYY„Äç„ÇíÂøÖ„Åö‰ªòÂä†„ÄÇ‰∏çÊòéÂÅ¥„ÅØ„Äå‰∏çÊòé„ÄêOCRÊ≥®ÊÑè„Äë„Äç„Å®ÊòéË®ò„ÄÇ
- Êû†„Åî„Å®„ÅÆÂÄüÊñπÈáëÈ°çÂêàË®à„Å®Ë≤∏ÊñπÈáëÈ°çÂêàË®à„ÅØ‰∏ÄËá¥„Åï„Åõ„Çã„ÄÇÂêà„Çè„Å™„ÅÑJSON„ÇíÂá∫Âäõ„Åó„Å¶„ÅØ„Å™„Çâ„Å™„ÅÑ„ÄÇ
  ‰∏çË∂≥ÂàÜ„ÅØ"‰∏çÊòé"„É¨„ÉÉ„Ç∞„ÇíËøΩÂä†„Åó„Å¶Âêà„Çè„Åõ„ÇãÔºàÁßëÁõÆÂêç„ÅØÁ©∫ÊñáÂ≠ó„ÅßÂèØÔºâ„ÄÇ

„ÄêÂÖ±ÈÄöÊëòË¶Å„É´„Éº„É´„Äë
- Áâ©‰ª∂Âêç„ÉªÂè∑ÂÆ§„ÉªÂ•ëÁ¥ÑËÄÖÂêç„Éª„Ç™„Éº„Éä„ÉºÂêç„Éª‚óãÊúàÂàÜË≥ÉÊñô„Å™„Å©„ÄÅ‰∏≠Â§Æ„ÅÆÊû†Âçò‰Ωç„ÅÆÊÉÖÂ†±„ÅØ„ÄÅÂêåÊû†„ÅÆÂÖ®„É¨„ÉÉ„Ç∞„Å´Á∂ôÊâø„Åô„Çã„ÄÇ
- Êû†ÂÜÖ„Åß‰∏ç‰∏ÄËá¥„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØÊÉÖÂ†±Èáè„ÅÆÂ§ö„ÅÑ„ÇÇ„ÅÆ„ÇíÊé°Áî®„ÄÇ‰∏çÁ¢∫ÂÆü„Å™„ÇâÊëòË¶ÅÊú´Â∞æ„Å´„ÄêOCRÊ≥®ÊÑè:ÁõÆË¶ñÁ¢∫Ë™çÊé®Â•®„Äë„ÄÇ

„Äê„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ„Äë
- ÊäΩÂá∫‰∏çËÉΩ„Åß„ÇÇÈÖçÂàó1Ë¶ÅÁ¥†„ÅØÂøÖ„ÅöËøî„ÅôÔºàÁßëÁõÆÂêçÁ©∫„ÄÅÊëòË¶ÅÊú´Â∞æ„Å´„ÄêOCRÊ≥®ÊÑè„ÄëÔºâ„ÄÇ
- ÈáëÈ°ç„Å´Ë≤†Êï∞„ÇÑË®òÂè∑„ÅØ‰Ωø„Çè„Å™„ÅÑ„ÄÇ
- Âá∫Âäõ‰æã: [{"‰ºùÁ•®Êó•‰ªò":"","ÂÄüË≤∏Âå∫ÂàÜ":"ÂÄüÊñπ","ÁßëÁõÆÂêç":"","ÈáëÈ°ç":0,"ÊëòË¶Å":"ÊäΩÂá∫‰∏çËÉΩ„ÄêOCRÊ≥®ÊÑè„Äë"}]"""
    
    def process_single_pdf(self, pdf_path: Path, temp_dir: Path, pages_per_split: Optional[int] = None) -> ProcessingResult:
        """
        Process single PDF with comprehensive error handling
        
        Args:
            pdf_path: PDF file path
            temp_dir: Temporary directory
            pages_per_split: Pages per split (uses config if None)
            
        Returns:
            ProcessingResult with detailed information
        """
        start_time = time.time()
        pages_per_split = pages_per_split or self.pages_per_split
        
        logger.info(f"Starting PDF processing: {pdf_path.name}")
        
        # Validate file
        is_valid, error_msg = self.validate_file(pdf_path)
        if not is_valid:
            return ProcessingResult(
                success=False,
                error_message=error_msg,
                file_path=str(pdf_path),
                processing_time=time.time() - start_time
            )
        
        # Backup file if enabled
        backup_path = self.backup_file(pdf_path)
        
        all_extracted_data = []
        split_files = []
        
        try:
            # Create PDF-specific temp directory
            pdf_temp_dir = temp_dir / pdf_path.stem
            pdf_temp_dir.mkdir(parents=True, exist_ok=True)
            
            # Split PDF
            split_files = self.split_pdf(pdf_path, pdf_temp_dir, pages_per_split)
            
            if not split_files:
                raise ValueError("No split files were created")
            
            # Process each split with threading if enabled
            if self.max_concurrent_requests > 1:
                all_extracted_data = self._process_splits_concurrent(split_files, pages_per_split)
            else:
                all_extracted_data = self._process_splits_sequential(split_files, pages_per_split)
            
            # Update statistics
            self.stats['files_processed'] += 1
            self.stats['pages_processed'] += self.get_pdf_page_count(pdf_path)
            processing_time = time.time() - start_time
            self.stats['total_processing_time'] += processing_time
            
            logger.info(f"PDF processing completed: {pdf_path.name} - {len(all_extracted_data)} entries in {processing_time:.2f}s")
            
            # ÁèæÂú®„ÅÆ„Éï„Ç°„Ç§„É´Âá¶ÁêÜ„ÅÆË≤ªÁî®Ë®àÁÆóÔºàÊúÄÊñ∞„É¨„Éº„Éà‰ΩøÁî®Ôºâ
            current_usd_rate = config.get_current_usd_to_jpy_rate()
            # Áµ±Ë®à„ÅÆË≤ªÁî®„ÇíÊúÄÊñ∞„É¨„Éº„Éà„ÅßÂÜçË®àÁÆó
            recalculated_cost_jpy = self.stats['total_cost_usd'] * current_usd_rate
            
            return ProcessingResult(
                success=True,
                data=all_extracted_data,
                file_path=str(pdf_path),
                processing_time=processing_time,
                pages_processed=len(split_files) * pages_per_split,
                total_cost_usd=self.stats['total_cost_usd'],
                total_cost_jpy=recalculated_cost_jpy
            )
            
        except Exception as e:
            error_msg = f"PDF processing failed: {str(e)}"
            logger.error(error_msg, exc_info=True)
            self.stats['errors_encountered'] += 1
            
            return ProcessingResult(
                success=False,
                error_message=error_msg,
                file_path=str(pdf_path),
                processing_time=time.time() - start_time
            )
            
        finally:
            # Cleanup split files
            if config.CLEANUP_TEMP_FILES:
                for split_file in split_files:
                    try:
                        if split_file.exists():
                            split_file.unlink()
                    except Exception as e:
                        logger.warning(f"Failed to cleanup {split_file}: {e}")
            
            # Memory cleanup
            if self.stats['files_processed'] % config.GARBAGE_COLLECTION_INTERVAL == 0:
                gc.collect()
    
    def _process_splits_sequential(self, split_files: List[Path], pages_per_split: int) -> List[Dict]:
        """Process split files sequentially"""
        all_data = []
        
        for i, split_file in enumerate(split_files):
            # Extract page range from filename (e.g., "file_pages_1-5.pdf" -> start=1, end=5)
            filename = split_file.stem
            if '_pages_' in filename:
                try:
                    page_range_str = filename.split('_pages_')[1]
                    start_str, end_str = page_range_str.split('-')
                    page_start = int(start_str)
                    page_end = int(end_str)
                except (ValueError, IndexError):
                    # Fallback to calculation method
                    page_start = i * pages_per_split + 1
                    page_end = (i + 1) * pages_per_split
            else:
                page_start = i * pages_per_split + 1
                page_end = (i + 1) * pages_per_split
            
            # Rate limiting (Êú¨Áï™API‰ΩøÁî®ÊôÇ)
            if i > 0:
                time.sleep(self.api_interval)
            
            extracted_data = self.extract_with_retry(split_file, page_start, page_end)
            all_data.extend(extracted_data)
            
            logger.info(f"Split processed: {split_file.name} - {len(extracted_data)} entries")
        
        return all_data
    
    def _process_splits_concurrent(self, split_files: List[Path], pages_per_split: int) -> List[Dict]:
        """Process split files concurrently with rate limiting"""
        all_data = []
        
        with ThreadPoolExecutor(max_workers=self.max_concurrent_requests) as executor:
            # Submit all tasks
            futures = []
            for i, split_file in enumerate(split_files):
                # Extract page range from filename (e.g., "file_pages_1-5.pdf" -> start=1, end=5)
                filename = split_file.stem
                if '_pages_' in filename:
                    try:
                        page_range_str = filename.split('_pages_')[1]
                        start_str, end_str = page_range_str.split('-')
                        page_start = int(start_str)
                        page_end = int(end_str)
                    except (ValueError, IndexError):
                        # Fallback to calculation method
                        page_start = i * pages_per_split + 1
                        page_end = (i + 1) * pages_per_split
                else:
                    page_start = i * pages_per_split + 1
                    page_end = (i + 1) * pages_per_split
                
                future = executor.submit(self.extract_with_retry, split_file, page_start, page_end)
                futures.append((future, split_file.name))
                
                # Rate limiting for concurrent requests (Êú¨Áï™API‰ΩøÁî®ÊôÇ)
                time.sleep(self.api_interval / self.max_concurrent_requests)
            
            # Collect results
            for future, filename in futures:
                try:
                    extracted_data = future.result(timeout=self.request_timeout * 2)
                    all_data.extend(extracted_data)
                    logger.info(f"Split processed: {filename} - {len(extracted_data)} entries")
                except Exception as e:
                    logger.error(f"Split processing failed: {filename} - {e}")
                    continue
        
        return all_data
    
    def get_pdf_page_count(self, pdf_path: Path) -> int:
        """Get PDF page count with error handling"""
        try:
            with open(pdf_path, 'rb') as file:
                reader = PdfReader(file)
                return len(reader.pages)
        except Exception as e:
            logger.error(f"Failed to get page count for {pdf_path}: {e}")
            return 0
    
    def process_directory(self, input_dir: Path, output_csv: Path, temp_dir: Path) -> bool:
        """
        Process directory with enhanced error handling and reporting
        
        Args:
            input_dir: Input directory
            output_csv: Output CSV file
            temp_dir: Temporary directory
            
        Returns:
            Success status
        """
        start_time = time.time()
        logger.info(f"Starting directory processing: {input_dir}")
        
        # Validate input directory
        if not input_dir.exists():
            logger.error(f"Input directory does not exist: {input_dir}")
            return False
        
        # Create output directories
        output_csv.parent.mkdir(parents=True, exist_ok=True)
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        # Get PDF files
        pdf_files = list(input_dir.glob("*.pdf"))
        if not pdf_files:
            logger.warning(f"No PDF files found in: {input_dir}")
            return True  # Not an error, just empty directory
        
        logger.info(f"Found {len(pdf_files)} PDF files to process")
        
        # Process files
        all_data = []
        successful_files = 0
        failed_files = []
        
        for pdf_file in pdf_files:
            logger.info(f"Processing: {pdf_file.name}")
            
            try:
                result = self.process_single_pdf(pdf_file, temp_dir)
                
                if result.success:
                    all_data.extend(result.data)
                    successful_files += 1
                    logger.info(f"Completed: {pdf_file.name} - {len(result.data)} entries")
                else:
                    failed_files.append((pdf_file.name, result.error_message))
                    logger.error(f"Failed: {pdf_file.name} - {result.error_message}")
                    
            except Exception as e:
                error_msg = f"Unexpected error processing {pdf_file.name}: {e}"
                logger.error(error_msg, exc_info=True)
                failed_files.append((pdf_file.name, error_msg))
        
        # Save results
        try:
            self.save_to_csv(all_data, output_csv)
            
            # Generate processing report
            self._generate_processing_report(
                start_time, len(pdf_files), successful_files, 
                failed_files, len(all_data), output_csv
            )
            
            logger.info(f"Directory processing completed: {successful_files}/{len(pdf_files)} files successful")
            return len(failed_files) == 0
            
        except Exception as e:
            logger.error(f"Failed to save results: {e}")
            return False
        
        finally:
            # Cleanup temp directory
            if config.CLEANUP_TEMP_FILES:
                self.cleanup_temp_dir(temp_dir)
    
    def _generate_processing_report(self, start_time: float, total_files: int, successful_files: int,
                                  failed_files: List[Tuple[str, str]], total_entries: int, output_csv: Path):
        """Generate detailed processing report"""
        processing_time = time.time() - start_time
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'processing_time_seconds': processing_time,
            'total_files': total_files,
            'successful_files': successful_files,
            'failed_files': len(failed_files),
            'total_entries_extracted': total_entries,
            'output_file': str(output_csv),
            'statistics': self.stats.copy(),
            'failed_file_details': [{'filename': name, 'error': error} for name, error in failed_files]
        }
        
        # Save report
        report_file = config.LOG_DIR / f"processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Processing report saved: {report_file}")
        
        # Print summary
        print(f"\n{'='*60}")
        print("üìä PROCESSING SUMMARY")
        print(f"{'='*60}")
        print(f"üïê Processing Time: {processing_time:.2f} seconds")
        print(f"üìÅ Total Files: {total_files}")
        print(f"‚úÖ Successful: {successful_files}")
        print(f"‚ùå Failed: {len(failed_files)}")
        print(f"üìÑ Total Entries: {total_entries}")
        print(f"üìä API Calls Made: {self.stats['api_calls_made']}")
        if failed_files:
            print(f"\n‚ùå Failed Files:")
            for filename, error in failed_files:
                print(f"   - {filename}: {error[:100]}...")
        print(f"{'='*60}")
    
    def save_to_csv(self, data: List[Dict], output_path: Path) -> None:
        """Save data to CSV with enhanced error handling"""
        if not data:
            logger.warning("No data to save")
            # Create empty CSV with 5-column headers (for internal use only)
            df = pd.DataFrame(columns=config.REQUIRED_JSON_FIELDS)
        else:
            df = pd.DataFrame(data)
            
            # Ensure all required 5-column fields exist
            for col in config.REQUIRED_JSON_FIELDS:
                if col not in df.columns:
                    df[col] = "" if col != "ÈáëÈ°ç" else 0
        
        try:
            # Save with BOM for Excel compatibility
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            logger.info(f"CSV saved: {output_path} ({len(data)} entries)")
            
        except Exception as e:
            logger.error(f"CSV save failed: {e}")
            raise
    
    def cleanup_temp_dir(self, temp_dir: Path) -> None:
        """Cleanup temporary directory with enhanced error handling"""
        try:
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
                logger.info(f"Temp directory cleaned: {temp_dir}")
        except Exception as e:
            logger.warning(f"Failed to cleanup temp directory: {e}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get processing statistics"""
        stats = self.stats.copy()
        stats['average_processing_time'] = (
            stats['total_processing_time'] / stats['files_processed'] 
            if stats['files_processed'] > 0 else 0
        )
        return stats

# Legacy compatibility wrapper
class PDFExtractor(ProductionPDFExtractor):
    """Legacy compatibility wrapper"""
    
    def __init__(self, api_key: str = "DUMMY_API_KEY", use_mock: bool = True):
        super().__init__(api_key=api_key, use_mock=use_mock)
    
    def extract_from_pdf_chunk(self, pdf_path: Path, page_start: int, page_end: int) -> List[Dict]:
        """Legacy compatibility method"""
        return self.extract_with_retry(pdf_path, page_start, page_end)

def main():
    """Main function for direct execution"""
    from config import print_config_summary, validate_config
    
    print_config_summary()
    
    if not validate_config():
        logger.error("Configuration validation failed")
        return False
    
    # Use configuration values
    extractor = ProductionPDFExtractor(
        api_key=config.OPENAI_API_KEY or config.ANTHROPIC_API_KEY,
        use_mock=config.USE_MOCK_DATA,
        api_provider='openai' if config.OPENAI_API_KEY else 'anthropic'
    )
    
    # Process directory
    success = extractor.process_directory(
        config.INPUT_DIR,
        config.OUTPUT_CSV_PATH,
        config.TEMP_DIR
    )
    
    # Print statistics
    stats = extractor.get_statistics()
    print(f"\nüìä Final Statistics:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    return success

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)